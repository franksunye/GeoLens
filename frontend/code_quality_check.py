#!/usr/bin/env python3
"""
GeoLens Frontend ä»£ç è´¨é‡æ£€æŸ¥è„šæœ¬
åˆ†æä»£ç è´¨é‡ã€æ€§èƒ½é—®é¢˜å’Œæœ€ä½³å®è·µ
"""

import os
import re
import ast
from pathlib import Path
from typing import Dict, List, Any, Set
import json

class CodeQualityAnalyzer:
    """ä»£ç è´¨é‡åˆ†æå™¨"""
    
    def __init__(self, project_root: str):
        self.project_root = Path(project_root)
        self.issues = []
        self.metrics = {
            'total_files': 0,
            'total_lines': 0,
            'total_functions': 0,
            'total_classes': 0,
            'duplicated_code_blocks': 0,
            'long_functions': 0,
            'complex_functions': 0,
            'missing_docstrings': 0
        }
    
    def analyze_project(self) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªé¡¹ç›®"""
        print("ğŸ” å¼€å§‹ä»£ç è´¨é‡åˆ†æ...")
        
        # è·å–æ‰€æœ‰Pythonæ–‡ä»¶
        python_files = list(self.project_root.glob("**/*.py"))
        self.metrics['total_files'] = len(python_files)
        
        for file_path in python_files:
            if self._should_skip_file(file_path):
                continue
                
            self._analyze_file(file_path)
        
        # æ£€æŸ¥é‡å¤ä»£ç 
        self._check_code_duplication(python_files)
        
        return {
            'metrics': self.metrics,
            'issues': self.issues,
            'recommendations': self._generate_recommendations()
        }
    
    def _should_skip_file(self, file_path: Path) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡æ–‡ä»¶"""
        skip_patterns = [
            '__pycache__',
            '.git',
            'venv',
            'env',
            '.pytest_cache',
            'test_',
            '__init__.py'
        ]
        
        return any(pattern in str(file_path) for pattern in skip_patterns)
    
    def _analyze_file(self, file_path: Path):
        """åˆ†æå•ä¸ªæ–‡ä»¶"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # åŸºæœ¬æŒ‡æ ‡
            lines = content.split('\n')
            self.metrics['total_lines'] += len(lines)
            
            # ASTåˆ†æ
            try:
                tree = ast.parse(content)
                self._analyze_ast(tree, file_path)
            except SyntaxError as e:
                self.issues.append({
                    'type': 'syntax_error',
                    'file': str(file_path),
                    'line': e.lineno,
                    'message': f"è¯­æ³•é”™è¯¯: {e.msg}",
                    'severity': 'error'
                })
            
            # æ–‡æœ¬åˆ†æ
            self._analyze_text(content, file_path)
            
        except Exception as e:
            self.issues.append({
                'type': 'file_error',
                'file': str(file_path),
                'message': f"æ–‡ä»¶åˆ†æå¤±è´¥: {str(e)}",
                'severity': 'warning'
            })
    
    def _analyze_ast(self, tree: ast.AST, file_path: Path):
        """åˆ†æAST"""
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                self._analyze_function(node, file_path)
            elif isinstance(node, ast.ClassDef):
                self._analyze_class(node, file_path)
    
    def _analyze_function(self, node: ast.FunctionDef, file_path: Path):
        """åˆ†æå‡½æ•°"""
        self.metrics['total_functions'] += 1
        
        # æ£€æŸ¥å‡½æ•°é•¿åº¦
        if hasattr(node, 'end_lineno') and hasattr(node, 'lineno'):
            func_length = node.end_lineno - node.lineno
            if func_length > 50:
                self.metrics['long_functions'] += 1
                self.issues.append({
                    'type': 'long_function',
                    'file': str(file_path),
                    'line': node.lineno,
                    'function': node.name,
                    'length': func_length,
                    'message': f"å‡½æ•° '{node.name}' è¿‡é•¿ ({func_length} è¡Œ)",
                    'severity': 'warning'
                })
        
        # æ£€æŸ¥å‡½æ•°å¤æ‚åº¦ï¼ˆç®€å•çš„åœˆå¤æ‚åº¦ï¼‰
        complexity = self._calculate_complexity(node)
        if complexity > 10:
            self.metrics['complex_functions'] += 1
            self.issues.append({
                'type': 'complex_function',
                'file': str(file_path),
                'line': node.lineno,
                'function': node.name,
                'complexity': complexity,
                'message': f"å‡½æ•° '{node.name}' å¤æ‚åº¦è¿‡é«˜ ({complexity})",
                'severity': 'warning'
            })
        
        # æ£€æŸ¥æ–‡æ¡£å­—ç¬¦ä¸²
        if not ast.get_docstring(node):
            self.metrics['missing_docstrings'] += 1
            self.issues.append({
                'type': 'missing_docstring',
                'file': str(file_path),
                'line': node.lineno,
                'function': node.name,
                'message': f"å‡½æ•° '{node.name}' ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²",
                'severity': 'info'
            })
    
    def _analyze_class(self, node: ast.ClassDef, file_path: Path):
        """åˆ†æç±»"""
        self.metrics['total_classes'] += 1
        
        # æ£€æŸ¥ç±»æ–‡æ¡£å­—ç¬¦ä¸²
        if not ast.get_docstring(node):
            self.issues.append({
                'type': 'missing_docstring',
                'file': str(file_path),
                'line': node.lineno,
                'class': node.name,
                'message': f"ç±» '{node.name}' ç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²",
                'severity': 'info'
            })
    
    def _calculate_complexity(self, node: ast.FunctionDef) -> int:
        """è®¡ç®—å‡½æ•°å¤æ‚åº¦"""
        complexity = 1  # åŸºç¡€å¤æ‚åº¦
        
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, (ast.And, ast.Or)):
                complexity += 1
        
        return complexity
    
    def _analyze_text(self, content: str, file_path: Path):
        """æ–‡æœ¬åˆ†æ"""
        lines = content.split('\n')
        
        for i, line in enumerate(lines, 1):
            # æ£€æŸ¥è¡Œé•¿åº¦
            if len(line) > 120:
                self.issues.append({
                    'type': 'long_line',
                    'file': str(file_path),
                    'line': i,
                    'length': len(line),
                    'message': f"è¡Œè¿‡é•¿ ({len(line)} å­—ç¬¦)",
                    'severity': 'info'
                })
            
            # æ£€æŸ¥TODO/FIXMEæ³¨é‡Š
            if re.search(r'#\s*(TODO|FIXME|XXX)', line, re.IGNORECASE):
                self.issues.append({
                    'type': 'todo_comment',
                    'file': str(file_path),
                    'line': i,
                    'message': "å‘ç°TODO/FIXMEæ³¨é‡Š",
                    'severity': 'info'
                })
            
            # æ£€æŸ¥printè¯­å¥ï¼ˆå¯èƒ½æ˜¯è°ƒè¯•ä»£ç ï¼‰
            if re.search(r'\bprint\s*\(', line) and 'logger' not in line:
                self.issues.append({
                    'type': 'debug_print',
                    'file': str(file_path),
                    'line': i,
                    'message': "å‘ç°printè¯­å¥ï¼Œå»ºè®®ä½¿ç”¨æ—¥å¿—",
                    'severity': 'info'
                })
    
    def _check_code_duplication(self, python_files: List[Path]):
        """æ£€æŸ¥ä»£ç é‡å¤"""
        # ç®€å•çš„é‡å¤ä»£ç æ£€æµ‹
        code_blocks = {}
        
        for file_path in python_files:
            if self._should_skip_file(file_path):
                continue
                
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                # æ£€æŸ¥è¿ç»­çš„5è¡Œä»£ç 
                for i in range(len(lines) - 4):
                    block = ''.join(lines[i:i+5]).strip()
                    if len(block) > 100:  # åªæ£€æŸ¥æœ‰æ„ä¹‰çš„ä»£ç å—
                        block_hash = hash(block)
                        
                        if block_hash in code_blocks:
                            self.metrics['duplicated_code_blocks'] += 1
                            self.issues.append({
                                'type': 'code_duplication',
                                'file': str(file_path),
                                'line': i + 1,
                                'duplicate_file': code_blocks[block_hash]['file'],
                                'duplicate_line': code_blocks[block_hash]['line'],
                                'message': "å‘ç°é‡å¤ä»£ç å—",
                                'severity': 'warning'
                            })
                        else:
                            code_blocks[block_hash] = {
                                'file': str(file_path),
                                'line': i + 1
                            }
            
            except Exception:
                continue
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºæŒ‡æ ‡ç”Ÿæˆå»ºè®®
        if self.metrics['long_functions'] > 0:
            recommendations.append(
                f"å‘ç° {self.metrics['long_functions']} ä¸ªè¿‡é•¿å‡½æ•°ï¼Œå»ºè®®æ‹†åˆ†ä¸ºæ›´å°çš„å‡½æ•°"
            )
        
        if self.metrics['complex_functions'] > 0:
            recommendations.append(
                f"å‘ç° {self.metrics['complex_functions']} ä¸ªå¤æ‚å‡½æ•°ï¼Œå»ºè®®ç®€åŒ–é€»è¾‘æˆ–æ‹†åˆ†å‡½æ•°"
            )
        
        if self.metrics['missing_docstrings'] > 5:
            recommendations.append(
                f"å‘ç° {self.metrics['missing_docstrings']} ä¸ªç¼ºå°‘æ–‡æ¡£å­—ç¬¦ä¸²çš„å‡½æ•°/ç±»ï¼Œå»ºè®®æ·»åŠ æ–‡æ¡£"
            )
        
        if self.metrics['duplicated_code_blocks'] > 0:
            recommendations.append(
                f"å‘ç° {self.metrics['duplicated_code_blocks']} ä¸ªé‡å¤ä»£ç å—ï¼Œå»ºè®®æå–å…¬å…±å‡½æ•°"
            )
        
        # åŸºäºé—®é¢˜ç±»å‹ç”Ÿæˆå»ºè®®
        error_count = len([i for i in self.issues if i['severity'] == 'error'])
        warning_count = len([i for i in self.issues if i['severity'] == 'warning'])
        
        if error_count > 0:
            recommendations.append(f"å‘ç° {error_count} ä¸ªé”™è¯¯ï¼Œéœ€è¦ç«‹å³ä¿®å¤")
        
        if warning_count > 10:
            recommendations.append(f"å‘ç° {warning_count} ä¸ªè­¦å‘Šï¼Œå»ºè®®é€æ­¥æ”¹è¿›")
        
        return recommendations

def generate_report(analysis_result: Dict[str, Any]) -> str:
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    metrics = analysis_result['metrics']
    issues = analysis_result['issues']
    recommendations = analysis_result['recommendations']
    
    report = []
    report.append("# ğŸ” GeoLens Frontend ä»£ç è´¨é‡åˆ†ææŠ¥å‘Š")
    report.append("")
    report.append("## ğŸ“Š ä»£ç æŒ‡æ ‡")
    report.append("")
    report.append(f"- **æ€»æ–‡ä»¶æ•°**: {metrics['total_files']}")
    report.append(f"- **æ€»ä»£ç è¡Œæ•°**: {metrics['total_lines']}")
    report.append(f"- **å‡½æ•°æ•°é‡**: {metrics['total_functions']}")
    report.append(f"- **ç±»æ•°é‡**: {metrics['total_classes']}")
    report.append("")
    
    # è´¨é‡æŒ‡æ ‡
    report.append("## ğŸ¯ è´¨é‡æŒ‡æ ‡")
    report.append("")
    report.append(f"- **è¿‡é•¿å‡½æ•°**: {metrics['long_functions']}")
    report.append(f"- **å¤æ‚å‡½æ•°**: {metrics['complex_functions']}")
    report.append(f"- **ç¼ºå°‘æ–‡æ¡£**: {metrics['missing_docstrings']}")
    report.append(f"- **é‡å¤ä»£ç å—**: {metrics['duplicated_code_blocks']}")
    report.append("")
    
    # é—®é¢˜ç»Ÿè®¡
    error_count = len([i for i in issues if i['severity'] == 'error'])
    warning_count = len([i for i in issues if i['severity'] == 'warning'])
    info_count = len([i for i in issues if i['severity'] == 'info'])
    
    report.append("## ğŸš¨ é—®é¢˜ç»Ÿè®¡")
    report.append("")
    report.append(f"- **é”™è¯¯**: {error_count}")
    report.append(f"- **è­¦å‘Š**: {warning_count}")
    report.append(f"- **ä¿¡æ¯**: {info_count}")
    report.append("")
    
    # ä¸»è¦é—®é¢˜
    if issues:
        report.append("## ğŸ”§ ä¸»è¦é—®é¢˜")
        report.append("")
        
        # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
        for severity in ['error', 'warning']:
            severity_issues = [i for i in issues if i['severity'] == severity]
            if severity_issues:
                severity_name = "é”™è¯¯" if severity == 'error' else "è­¦å‘Š"
                report.append(f"### {severity_name}")
                report.append("")
                
                for issue in severity_issues[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    file_name = Path(issue['file']).name
                    line = issue.get('line', '?')
                    message = issue['message']
                    report.append(f"- **{file_name}:{line}** - {message}")
                
                if len(severity_issues) > 10:
                    report.append(f"- ... è¿˜æœ‰ {len(severity_issues) - 10} ä¸ª{severity_name}")
                
                report.append("")
    
    # æ”¹è¿›å»ºè®®
    if recommendations:
        report.append("## ğŸ’¡ æ”¹è¿›å»ºè®®")
        report.append("")
        for i, rec in enumerate(recommendations, 1):
            report.append(f"{i}. {rec}")
        report.append("")
    
    # æ€»ä½“è¯„åˆ†
    total_issues = len(issues)
    total_functions = metrics['total_functions']
    
    if total_functions > 0:
        quality_score = max(0, 100 - (total_issues / total_functions * 10))
        report.append("## ğŸ“ˆ è´¨é‡è¯„åˆ†")
        report.append("")
        report.append(f"**æ€»ä½“è´¨é‡è¯„åˆ†**: {quality_score:.1f}/100")
        report.append("")
        
        if quality_score >= 90:
            report.append("âœ… **ä¼˜ç§€** - ä»£ç è´¨é‡å¾ˆé«˜")
        elif quality_score >= 80:
            report.append("ğŸŸ¡ **è‰¯å¥½** - ä»£ç è´¨é‡è¾ƒå¥½ï¼Œæœ‰æ”¹è¿›ç©ºé—´")
        elif quality_score >= 70:
            report.append("ğŸŸ  **ä¸€èˆ¬** - ä»£ç è´¨é‡ä¸€èˆ¬ï¼Œå»ºè®®æ”¹è¿›")
        else:
            report.append("ğŸ”´ **éœ€è¦æ”¹è¿›** - ä»£ç è´¨é‡è¾ƒå·®ï¼Œéœ€è¦é‡ç‚¹æ”¹è¿›")
    
    return "\n".join(report)

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒ GeoLens Frontend ä»£ç è´¨é‡æ£€æŸ¥å·¥å…·")
    print("=" * 60)
    
    # åˆ†æä»£ç 
    analyzer = CodeQualityAnalyzer(".")
    result = analyzer.analyze_project()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generate_report(result)
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = Path("code_quality_report.md")
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    # è¾“å‡ºæ‘˜è¦
    print("\nğŸ“Š åˆ†æå®Œæˆï¼")
    print(f"ğŸ“ åˆ†ææ–‡ä»¶: {result['metrics']['total_files']}")
    print(f"ğŸ“ ä»£ç è¡Œæ•°: {result['metrics']['total_lines']}")
    print(f"ğŸ”§ å‘ç°é—®é¢˜: {len(result['issues'])}")
    print(f"ğŸ’¡ æ”¹è¿›å»ºè®®: {len(result['recommendations'])}")
    print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {report_file}")
    
    # ä¿å­˜JSONæ•°æ®
    json_file = Path("code_quality_data.json")
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2, default=str)
    
    print(f"ğŸ“Š æ•°æ®æ–‡ä»¶: {json_file}")

if __name__ == "__main__":
    main()
