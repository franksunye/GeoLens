"""
å¼•ç”¨æ£€æµ‹é¡µé¢
æ ¸å¿ƒåŠŸèƒ½ - AIå¼•ç”¨æ£€æµ‹
"""

import streamlit as st
import asyncio
import json
from datetime import datetime
from typing import List, Dict, Any

from components.auth import require_auth
from components.sidebar import render_sidebar
from services.api_client import APIClient
from services.detection_service import DetectionService
from components.charts import render_detection_results_chart, render_model_comparison_chart
from utils.session import get_current_project, set_detection_state, get_detection_state

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="å¼•ç”¨æ£€æµ‹ - GeoLens",
    page_icon="ðŸ”",
    layout="wide"
)

@require_auth
def main():
    """ä¸»å‡½æ•°"""
    render_sidebar()
    
    st.markdown("# ðŸ” AIå¼•ç”¨æ£€æµ‹")
    st.markdown("è¾“å…¥Promptï¼Œé€‰æ‹©å“ç‰Œå’ŒAIæ¨¡åž‹ï¼Œå¼€å§‹æ™ºèƒ½å¼•ç”¨æ£€æµ‹åˆ†æž")
    
    # æ£€æŸ¥å½“å‰é¡¹ç›®
    current_project = get_current_project()
    if not current_project:
        st.warning("âš ï¸ è¯·å…ˆé€‰æ‹©ä¸€ä¸ªé¡¹ç›®")
        if st.button("ðŸ“ å‰å¾€é¡¹ç›®ç®¡ç†"):
            st.switch_page("pages/2_ðŸ“_Projects.py")
        return
    
    # æ˜¾ç¤ºå½“å‰é¡¹ç›®ä¿¡æ¯
    with st.container():
        st.info(f"ðŸ“ å½“å‰é¡¹ç›®: **{current_project.get('name', 'æœªå‘½åé¡¹ç›®')}** | ðŸŒ {current_project.get('domain', '')}")
    
    # ä¸»è¦å†…å®¹åŒºåŸŸ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        render_detection_form()
    
    with col2:
        render_detection_status()
        render_quick_templates()
    
    # æ£€æµ‹ç»“æžœå±•ç¤º
    render_detection_results()

def render_detection_form():
    """æ¸²æŸ“æ£€æµ‹è¡¨å•"""
    st.markdown("### ðŸ“ æ£€æµ‹é…ç½®")
    
    with st.form("detection_form"):
        # Promptè¾“å…¥
        prompt = st.text_area(
            "ðŸŽ¯ æ£€æµ‹Prompt",
            height=120,
            placeholder="ä¾‹å¦‚: æŽ¨èå‡ ä¸ªå¥½ç”¨çš„å›¢é˜Ÿåä½œå’Œç¬”è®°ç®¡ç†å·¥å…·",
            help="è¾“å…¥æ‚¨æƒ³è¦æ£€æµ‹çš„é—®é¢˜æˆ–åœºæ™¯"
        )
        
        # å“ç‰Œé€‰æ‹©
        col1, col2 = st.columns(2)
        
        with col1:
            # ä»Žå½“å‰é¡¹ç›®èŽ·å–å“ç‰Œåˆ—è¡¨
            current_project = get_current_project()
            available_brands = current_project.get('brands', [
                "Notion", "Obsidian", "Roam Research", 
                "Slack", "Teams", "Discord",
                "Figma", "Sketch", "Adobe XD"
            ])
            
            selected_brands = st.multiselect(
                "ðŸ·ï¸ é€‰æ‹©å“ç‰Œ",
                options=available_brands,
                default=st.session_state.get('selected_brands', []),
                help="é€‰æ‹©è¦æ£€æµ‹çš„å“ç‰Œ"
            )
        
        with col2:
            # AIæ¨¡åž‹é€‰æ‹©
            available_models = ["doubao", "deepseek", "openai"]
            selected_models = st.multiselect(
                "ðŸ¤– é€‰æ‹©AIæ¨¡åž‹",
                options=available_models,
                default=st.session_state.get('selected_models', ["doubao", "deepseek"]),
                help="é€‰æ‹©ç”¨äºŽæ£€æµ‹çš„AIæ¨¡åž‹"
            )
        
        # é«˜çº§é…ç½®
        with st.expander("âš™ï¸ é«˜çº§é…ç½®", expanded=False):
            col1, col2, col3 = st.columns(3)
            
            with col1:
                max_tokens = st.number_input(
                    "æœ€å¤§Tokenæ•°",
                    min_value=100,
                    max_value=1000,
                    value=300,
                    step=50
                )
            
            with col2:
                temperature = st.slider(
                    "æ¸©åº¦å‚æ•°",
                    min_value=0.0,
                    max_value=1.0,
                    value=0.3,
                    step=0.1
                )
            
            with col3:
                parallel_execution = st.checkbox(
                    "å¹¶è¡Œæ‰§è¡Œ",
                    value=True,
                    help="åŒæ—¶è°ƒç”¨å¤šä¸ªAIæ¨¡åž‹"
                )
        
        # æäº¤æŒ‰é’®
        col1, col2, col3 = st.columns([1, 1, 1])
        with col2:
            submit_button = st.form_submit_button(
                "ðŸš€ å¼€å§‹æ£€æµ‹",
                type="primary",
                use_container_width=True
            )
    
    # å¤„ç†è¡¨å•æäº¤
    if submit_button:
        if not prompt.strip():
            st.error("âŒ è¯·è¾“å…¥æ£€æµ‹Prompt")
            return
        
        if not selected_brands:
            st.error("âŒ è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªå“ç‰Œ")
            return
        
        if not selected_models:
            st.error("âŒ è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªAIæ¨¡åž‹")
            return
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        st.session_state.selected_brands = selected_brands
        st.session_state.selected_models = selected_models
        
        # æ‰§è¡Œæ£€æµ‹
        run_detection(
            prompt=prompt,
            brands=selected_brands,
            models=selected_models,
            max_tokens=max_tokens,
            temperature=temperature,
            parallel_execution=parallel_execution
        )

def run_detection(prompt: str, brands: List[str], models: List[str], 
                 max_tokens: int, temperature: float, parallel_execution: bool):
    """æ‰§è¡Œæ£€æµ‹"""
    
    # è®¾ç½®æ£€æµ‹çŠ¶æ€
    set_detection_state(running=True)
    
    # æ˜¾ç¤ºè¿›åº¦
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # å‡†å¤‡æ£€æµ‹å‚æ•°
        current_project = get_current_project()
        detection_params = {
            "project_id": current_project.get('id', 'demo-project'),
            "prompt": prompt,
            "brands": brands,
            "models": models,
            "max_tokens": max_tokens,
            "temperature": temperature,
            "parallel_execution": parallel_execution
        }
        
        # æ¨¡æ‹Ÿæ£€æµ‹è¿‡ç¨‹
        status_text.text("ðŸ”„ æ­£åœ¨åˆå§‹åŒ–æ£€æµ‹...")
        progress_bar.progress(10)
        
        # è°ƒç”¨æ£€æµ‹æœåŠ¡
        detection_service = DetectionService()
        
        status_text.text("ðŸ¤– æ­£åœ¨è°ƒç”¨AIæ¨¡åž‹...")
        progress_bar.progress(30)
        
        # æ¨¡æ‹ŸAPIè°ƒç”¨
        import time
        time.sleep(2)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        
        status_text.text("ðŸ” æ­£åœ¨åˆ†æžæ£€æµ‹ç»“æžœ...")
        progress_bar.progress(70)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿç»“æžœ
        results = generate_mock_detection_results(prompt, brands, models)
        
        status_text.text("âœ… æ£€æµ‹å®Œæˆï¼")
        progress_bar.progress(100)
        
        # ä¿å­˜ç»“æžœ
        set_detection_state(running=False, result=results)
        
        st.success("ðŸŽ‰ æ£€æµ‹å®Œæˆï¼è¯·æŸ¥çœ‹ä¸‹æ–¹ç»“æžœ")
        
    except Exception as e:
        st.error(f"âŒ æ£€æµ‹è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        set_detection_state(running=False)
    
    finally:
        # æ¸…ç†è¿›åº¦æ˜¾ç¤º
        progress_bar.empty()
        status_text.empty()

def generate_mock_detection_results(prompt: str, brands: List[str], models: List[str]) -> Dict[str, Any]:
    """ç”Ÿæˆæ¨¡æ‹Ÿæ£€æµ‹ç»“æžœ"""
    import random
    
    results = {
        "check_id": f"check_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
        "prompt": prompt,
        "brands_checked": brands,
        "models_used": models,
        "status": "completed",
        "created_at": datetime.now().isoformat(),
        "total_mentions": 0,
        "mention_rate": 0.0,
        "avg_confidence": 0.0,
        "model_results": [],
        "brand_mentions": []
    }
    
    # ä¸ºæ¯ä¸ªæ¨¡åž‹ç”Ÿæˆç»“æžœ
    for model in models:
        model_result = {
            "model": model,
            "response_text": f"åŸºäºŽ{model}æ¨¡åž‹çš„å›žç­”: è¿™é‡Œæ˜¯å…³äºŽ{prompt}çš„è¯¦ç»†å›žç­”...",
            "processing_time_ms": random.randint(2000, 8000),
            "mentions": []
        }
        
        # ä¸ºæ¯ä¸ªå“ç‰Œç”ŸæˆæåŠç»“æžœ
        for brand in brands:
            mentioned = random.choice([True, False, False])  # 33%æ¦‚çŽ‡è¢«æåŠ
            if mentioned:
                mention = {
                    "brand": brand,
                    "mentioned": True,
                    "confidence_score": round(random.uniform(0.7, 0.95), 2),
                    "context_snippet": f"...æŽ¨èä½¿ç”¨{brand}ï¼Œå®ƒæ˜¯ä¸€ä¸ªä¼˜ç§€çš„å·¥å…·...",
                    "position": random.randint(50, 200)
                }
                model_result["mentions"].append(mention)
                results["brand_mentions"].append(mention)
        
        results["model_results"].append(model_result)
    
    # è®¡ç®—æ€»ä½“ç»Ÿè®¡
    total_mentions = len(results["brand_mentions"])
    results["total_mentions"] = total_mentions
    results["mention_rate"] = round(total_mentions / (len(brands) * len(models)) * 100, 1)
    
    if results["brand_mentions"]:
        avg_confidence = sum(m["confidence_score"] for m in results["brand_mentions"]) / len(results["brand_mentions"])
        results["avg_confidence"] = round(avg_confidence, 2)
    
    return results

def render_detection_status():
    """æ¸²æŸ“æ£€æµ‹çŠ¶æ€"""
    st.markdown("### ðŸ“Š æ£€æµ‹çŠ¶æ€")
    
    running, last_result = get_detection_state()
    
    if running:
        st.info("ðŸ”„ æ£€æµ‹è¿›è¡Œä¸­...")
    elif last_result:
        st.success("âœ… æœ€è¿‘æ£€æµ‹å®Œæˆ")
        
        # æ˜¾ç¤ºç®€è¦ç»Ÿè®¡
        col1, col2 = st.columns(2)
        with col1:
            st.metric("æåŠæ¬¡æ•°", last_result.get("total_mentions", 0))
        with col2:
            st.metric("æåŠçŽ‡", f"{last_result.get('mention_rate', 0)}%")
        
        if st.button("ðŸ“Š æŸ¥çœ‹è¯¦ç»†ç»“æžœ"):
            st.rerun()
    else:
        st.info("ðŸ’¡ å°šæœªè¿›è¡Œæ£€æµ‹")

def render_quick_templates():
    """æ¸²æŸ“å¿«é€Ÿæ¨¡æ¿"""
    st.markdown("### ðŸ“š å¿«é€Ÿæ¨¡æ¿")
    
    templates = [
        {
            "name": "ç¬”è®°è½¯ä»¶æŽ¨è",
            "prompt": "æŽ¨èå‡ ä¸ªå¥½ç”¨çš„ç¬”è®°ç®¡ç†è½¯ä»¶",
            "brands": ["Notion", "Obsidian", "Roam Research"]
        },
        {
            "name": "å›¢é˜Ÿåä½œå·¥å…·",
            "prompt": "æ¯”è¾ƒä¸»æµçš„å›¢é˜Ÿåä½œå·¥å…·",
            "brands": ["Slack", "Teams", "Discord"]
        },
        {
            "name": "è®¾è®¡å·¥å…·å¯¹æ¯”",
            "prompt": "ä»‹ç»å‡ ä¸ªä¸“ä¸šçš„UIè®¾è®¡å·¥å…·",
            "brands": ["Figma", "Sketch", "Adobe XD"]
        }
    ]
    
    for template in templates:
        if st.button(
            template["name"],
            key=f"template_{template['name']}",
            help=f"Prompt: {template['prompt']}"
        ):
            # åº”ç”¨æ¨¡æ¿åˆ°è¡¨å•
            st.session_state.template_prompt = template["prompt"]
            st.session_state.selected_brands = template["brands"]
            st.rerun()

def render_detection_results():
    """æ¸²æŸ“æ£€æµ‹ç»“æžœ"""
    running, last_result = get_detection_state()
    
    if not last_result:
        return
    
    st.markdown("---")
    st.markdown("## ðŸ“Š æ£€æµ‹ç»“æžœ")
    
    # ç»“æžœæ¦‚è§ˆ
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric(
            "æ€»æåŠæ¬¡æ•°",
            last_result.get("total_mentions", 0),
            help="æ‰€æœ‰æ¨¡åž‹ä¸­å“ç‰Œè¢«æåŠçš„æ€»æ¬¡æ•°"
        )
    
    with col2:
        st.metric(
            "å¹³å‡æåŠçŽ‡",
            f"{last_result.get('mention_rate', 0)}%",
            help="å“ç‰Œè¢«æåŠçš„å¹³å‡æ¦‚çŽ‡"
        )
    
    with col3:
        st.metric(
            "å¹³å‡ç½®ä¿¡åº¦",
            f"{last_result.get('avg_confidence', 0):.2f}",
            help="æ£€æµ‹ç»“æžœçš„å¹³å‡ç½®ä¿¡åº¦"
        )
    
    with col4:
        st.metric(
            "æ£€æµ‹æ¨¡åž‹æ•°",
            len(last_result.get("models_used", [])),
            help="å‚ä¸Žæ£€æµ‹çš„AIæ¨¡åž‹æ•°é‡"
        )
    
    # è¯¦ç»†ç»“æžœå±•ç¤º
    tab1, tab2, tab3 = st.tabs(["ðŸ“Š å¯è§†åŒ–ç»“æžœ", "ðŸ“ è¯¦ç»†æ•°æ®", "ðŸ¤– æ¨¡åž‹å›žç­”"])
    
    with tab1:
        render_results_visualization(last_result)
    
    with tab2:
        render_results_table(last_result)
    
    with tab3:
        render_model_responses(last_result)

def render_results_visualization(results: Dict[str, Any]):
    """æ¸²æŸ“ç»“æžœå¯è§†åŒ–"""
    
    # å“ç‰ŒæåŠçŽ‡å›¾è¡¨
    if results.get("brand_mentions"):
        render_detection_results_chart(results["brand_mentions"])
    
    # æ¨¡åž‹æ€§èƒ½å¯¹æ¯”
    if results.get("model_results"):
        render_model_comparison_chart(results["model_results"])

def render_results_table(results: Dict[str, Any]):
    """æ¸²æŸ“ç»“æžœè¡¨æ ¼"""
    import pandas as pd
    
    if results.get("brand_mentions"):
        df = pd.DataFrame(results["brand_mentions"])
        st.dataframe(
            df,
            use_container_width=True,
            hide_index=True
        )
    else:
        st.info("ðŸ“ æš‚æ— å“ç‰ŒæåŠæ•°æ®")

def render_model_responses(results: Dict[str, Any]):
    """æ¸²æŸ“æ¨¡åž‹å›žç­”"""
    for model_result in results.get("model_results", []):
        with st.expander(f"ðŸ¤– {model_result['model'].title()} æ¨¡åž‹å›žç­”", expanded=False):
            st.markdown(f"**å¤„ç†æ—¶é—´**: {model_result['processing_time_ms']}ms")
            st.markdown("**å›žç­”å†…å®¹**:")
            st.text_area(
                "",
                value=model_result["response_text"],
                height=150,
                disabled=True,
                key=f"response_{model_result['model']}"
            )
            
            if model_result.get("mentions"):
                st.markdown("**æ£€æµ‹åˆ°çš„å“ç‰ŒæåŠ**:")
                for mention in model_result["mentions"]:
                    st.markdown(f"- **{mention['brand']}** (ç½®ä¿¡åº¦: {mention['confidence_score']:.2f})")
                    st.markdown(f"  > {mention['context_snippet']}")

if __name__ == "__main__":
    main()
