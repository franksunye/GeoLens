"""
ä¸šåŠ¡åœºæ™¯ç«¯åˆ°ç«¯æµ‹è¯•
"""

import pytest
import json
from typing import List, Dict, Any

from app.services.mention_detection import MentionDetectionService
from app.repositories.mention_repository import MentionRepository


@pytest.mark.e2e
class TestBusinessScenarios:
    """ä¸šåŠ¡åœºæ™¯æµ‹è¯•"""

    async def test_brand_monitoring_scenario(
        self,
        skip_if_no_api_keys,
        mention_service,
        test_project_data,
        e2e_db_session,
        e2e_config
    ):
        """å“ç‰Œç›‘æ§åœºæ™¯æµ‹è¯•"""
        print(f"ğŸ¢ å¼€å§‹å“ç‰Œç›‘æ§åœºæ™¯æµ‹è¯•")
        
        # æ¨¡æ‹Ÿå“ç‰Œç›‘æ§åœºæ™¯ï¼šç›‘æ§Notionåœ¨ä¸åŒæŸ¥è¯¢ä¸­çš„è¡¨ç°
        brand_to_monitor = "Notion"
        monitoring_queries = [
            "æ¨èå‡ ä¸ªå›¢é˜Ÿåä½œå·¥å…·",
            "æœ‰å“ªäº›å¥½ç”¨çš„ç¬”è®°è½¯ä»¶ï¼Ÿ",
            "çŸ¥è¯†ç®¡ç†ç³»ç»Ÿæœ‰ä»€ä¹ˆæ¨èï¼Ÿ",
            "é¡¹ç›®ç®¡ç†å·¥å…·æ¨è",
            "æ–‡æ¡£åä½œå¹³å°æœ‰å“ªäº›ï¼Ÿ"
        ]
        
        monitoring_results = []
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œç›‘æ§
        for i, query in enumerate(monitoring_queries):
            print(f"   ç›‘æ§æŸ¥è¯¢ {i+1}/{len(monitoring_queries)}: {query}")

            # åˆ›å»ºæ£€æµ‹é…ç½®
            from app.services.mention_detection import MentionDetectionConfig
            config = MentionDetectionConfig(
                models=["doubao", "deepseek"],
                api_keys={
                    "DOUBAO_API_KEY": e2e_config["doubao_api_key"],
                    "DEEPSEEK_API_KEY": e2e_config["deepseek_api_key"]
                },
                max_tokens=300,
                temperature=0.3,
                parallel_execution=False
            )

            result = await mention_service.execute_detection(
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"],
                prompt=query,
                brands=[brand_to_monitor],
                config=config
            )
            
            assert result.status == "completed"
            
            # æ”¶é›†ç›‘æ§æ•°æ®
            brand_mentions = []
            for model_result in result.results:
                for mention in model_result.mentions:
                    if mention.brand == brand_to_monitor:
                        brand_mentions.append({
                            "model": model_result.model,
                            "mentioned": mention.mentioned,
                            "confidence": mention.confidence_score,
                            "context": mention.context if hasattr(mention, 'context') else ""
                        })
            
            monitoring_results.append({
                "query": query,
                "check_id": result.check_id,
                "mentions": brand_mentions,
                "total_models": len(result.results)
            })
        
        # åˆ†æç›‘æ§ç»“æœ
        total_queries = len(monitoring_results)
        mentioned_count = sum(
            1 for result in monitoring_results 
            if any(mention["mentioned"] for mention in result["mentions"])
        )
        
        mention_rate = mentioned_count / total_queries if total_queries > 0 else 0
        
        # è®¡ç®—å¹³å‡ç½®ä¿¡åº¦
        all_confidences = []
        for result in monitoring_results:
            for mention in result["mentions"]:
                if mention["mentioned"]:
                    all_confidences.append(mention["confidence"])
        
        avg_confidence = sum(all_confidences) / len(all_confidences) if all_confidences else 0
        
        # éªŒè¯ç›‘æ§ç»“æœ
        assert total_queries == len(monitoring_queries)
        assert 0 <= mention_rate <= 1
        assert 0 <= avg_confidence <= 1
        
        print(f"âœ… å“ç‰Œç›‘æ§åœºæ™¯æµ‹è¯•å®Œæˆ")
        print(f"   ç›‘æ§å“ç‰Œ: {brand_to_monitor}")
        print(f"   æŸ¥è¯¢æ€»æ•°: {total_queries}")
        print(f"   è¢«æåŠæ¬¡æ•°: {mentioned_count}")
        print(f"   æåŠç‡: {mention_rate:.2%}")
        print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2f}")

    async def test_competitor_analysis_scenario(
        self,
        skip_if_no_api_keys,
        mention_service,
        test_project_data,
        e2e_db_session,
        e2e_config
    ):
        """ç«å“åˆ†æåœºæ™¯æµ‹è¯•"""
        print(f"ğŸ” å¼€å§‹ç«å“åˆ†æåœºæ™¯æµ‹è¯•")
        
        # ç«å“åˆ†æï¼šæ¯”è¾ƒå¤šä¸ªçŸ¥è¯†ç®¡ç†å·¥å…·
        competitors = ["Notion", "Obsidian", "Roam Research", "Logseq"]
        analysis_queries = [
            "æ¯”è¾ƒå‡ ä¸ªçŸ¥è¯†ç®¡ç†å·¥å…·çš„ä¼˜ç¼ºç‚¹",
            "å“ªä¸ªç¬”è®°è½¯ä»¶æœ€é€‚åˆç ”ç©¶äººå‘˜ï¼Ÿ",
            "å›¢é˜ŸçŸ¥è¯†åº“ç”¨ä»€ä¹ˆå·¥å…·æ¯”è¾ƒå¥½ï¼Ÿ"
        ]
        
        competitor_analysis = {brand: {"mentions": 0, "total_checks": 0, "confidences": []} 
                             for brand in competitors}
        
        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œç«å“åˆ†æ
        for i, query in enumerate(analysis_queries):
            print(f"   åˆ†ææŸ¥è¯¢ {i+1}/{len(analysis_queries)}: {query}")

            # åˆ›å»ºæ£€æµ‹é…ç½®
            from app.services.mention_detection import MentionDetectionConfig
            config = MentionDetectionConfig(
                models=["doubao"],  # ä½¿ç”¨å•ä¸ªæ¨¡å‹å‡å°‘APIè°ƒç”¨
                api_keys={"DOUBAO_API_KEY": e2e_config["doubao_api_key"]},
                max_tokens=300,
                temperature=0.3,
                parallel_execution=False
            )

            result = await mention_service.execute_detection(
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"],
                prompt=query,
                brands=competitors,
                config=config
            )
            
            assert result.status == "completed"
            assert len(result.results) == 1  # ä¸€ä¸ªæ¨¡å‹
            
            model_result = result.results[0]
            
            # åˆ†ææ¯ä¸ªç«å“çš„è¡¨ç°
            for mention in model_result.mentions:
                brand = mention.brand
                competitor_analysis[brand]["total_checks"] += 1
                
                if mention.mentioned:
                    competitor_analysis[brand]["mentions"] += 1
                    competitor_analysis[brand]["confidences"].append(mention.confidence_score)
        
        # è®¡ç®—ç«å“åˆ†ææŒ‡æ ‡
        analysis_summary = {}
        for brand, data in competitor_analysis.items():
            mention_rate = data["mentions"] / data["total_checks"] if data["total_checks"] > 0 else 0
            avg_confidence = (sum(data["confidences"]) / len(data["confidences"]) 
                            if data["confidences"] else 0)
            
            analysis_summary[brand] = {
                "mention_rate": mention_rate,
                "avg_confidence": avg_confidence,
                "total_mentions": data["mentions"],
                "total_checks": data["total_checks"]
            }
        
        # éªŒè¯åˆ†æç»“æœ
        for brand, summary in analysis_summary.items():
            assert 0 <= summary["mention_rate"] <= 1
            assert 0 <= summary["avg_confidence"] <= 1
            assert summary["total_checks"] == len(analysis_queries)
        
        # æ‰¾å‡ºè¡¨ç°æœ€å¥½çš„ç«å“
        best_performer = max(analysis_summary.items(), 
                           key=lambda x: (x[1]["mention_rate"], x[1]["avg_confidence"]))
        
        print(f"âœ… ç«å“åˆ†æåœºæ™¯æµ‹è¯•å®Œæˆ")
        print(f"   åˆ†æç«å“: {competitors}")
        print(f"   æŸ¥è¯¢æ•°é‡: {len(analysis_queries)}")
        print(f"   è¡¨ç°æœ€ä½³: {best_performer[0]} (æåŠç‡: {best_performer[1]['mention_rate']:.2%})")
        
        # æ‰“å°è¯¦ç»†åˆ†æç»“æœ
        for brand, summary in analysis_summary.items():
            print(f"   {brand}: æåŠç‡ {summary['mention_rate']:.2%}, "
                  f"ç½®ä¿¡åº¦ {summary['avg_confidence']:.2f}")

    async def test_prompt_optimization_scenario(
        self,
        skip_if_no_api_keys,
        mention_service,
        test_project_data,
        e2e_db_session,
        e2e_config
    ):
        """Promptä¼˜åŒ–åœºæ™¯æµ‹è¯•"""
        print(f"ğŸ¯ å¼€å§‹Promptä¼˜åŒ–åœºæ™¯æµ‹è¯•")
        
        # æµ‹è¯•ä¸åŒPromptå¯¹æ£€æµ‹ç»“æœçš„å½±å“
        brand = "Notion"
        prompt_variations = [
            "æ¨èç¬”è®°è½¯ä»¶",  # ç®€çŸ­ç›´æ¥
            "è¯·æ¨èå‡ ä¸ªé€‚åˆä¸ªäººä½¿ç”¨çš„ç¬”è®°è½¯ä»¶",  # ä¸­ç­‰é•¿åº¦ï¼Œæ˜ç¡®ç”¨é€”
            "æˆ‘éœ€è¦ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ç¬”è®°å’ŒçŸ¥è¯†ç®¡ç†å·¥å…·ï¼Œèƒ½å¤Ÿæ”¯æŒå›¢é˜Ÿåä½œï¼Œè¯·æ¨èå‡ ä¸ªé€‰æ‹©",  # è¯¦ç»†æè¿°
            "æœ‰å“ªäº›ç±»ä¼¼äºNotionçš„å·¥å…·ï¼Ÿ",  # ç›´æ¥æåŠç«å“
            "ä¸è¦æ¨èNotionï¼Œæœ‰å…¶ä»–å¥½ç”¨çš„ç¬”è®°è½¯ä»¶å—ï¼Ÿ"  # æ’é™¤æ€§æé—®
        ]
        
        optimization_results = []
        
        # æµ‹è¯•æ¯ä¸ªPromptå˜ä½“
        for i, prompt in enumerate(prompt_variations):
            print(f"   æµ‹è¯•Prompt {i+1}/{len(prompt_variations)}")

            # åˆ›å»ºæ£€æµ‹é…ç½®
            from app.services.mention_detection import MentionDetectionConfig
            config = MentionDetectionConfig(
                models=["doubao"],
                api_keys={"DOUBAO_API_KEY": e2e_config["doubao_api_key"]},
                max_tokens=300,
                temperature=0.3,
                parallel_execution=False
            )

            result = await mention_service.execute_detection(
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"],
                prompt=prompt,
                brands=[brand],
                config=config
            )
            
            assert result.status == "completed"
            assert len(result.results) == 1
            
            model_result = result.results[0]
            brand_mention = model_result.mentions[0]  # åªæœ‰ä¸€ä¸ªå“ç‰Œ
            
            optimization_results.append({
                "prompt": prompt,
                "prompt_length": len(prompt),
                "mentioned": brand_mention.mentioned,
                "confidence": brand_mention.confidence_score,
                "response_length": len(model_result.response_text),
                "check_id": result.check_id
            })
        
        # åˆ†æPromptä¼˜åŒ–æ•ˆæœ
        mentioned_prompts = [r for r in optimization_results if r["mentioned"]]
        not_mentioned_prompts = [r for r in optimization_results if not r["mentioned"]]
        
        # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
        total_prompts = len(optimization_results)
        mention_rate = len(mentioned_prompts) / total_prompts
        
        if mentioned_prompts:
            avg_confidence_when_mentioned = sum(r["confidence"] for r in mentioned_prompts) / len(mentioned_prompts)
            avg_prompt_length_when_mentioned = sum(r["prompt_length"] for r in mentioned_prompts) / len(mentioned_prompts)
        else:
            avg_confidence_when_mentioned = 0
            avg_prompt_length_when_mentioned = 0
        
        # éªŒè¯ä¼˜åŒ–ç»“æœ
        assert total_prompts == len(prompt_variations)
        assert 0 <= mention_rate <= 1
        
        print(f"âœ… Promptä¼˜åŒ–åœºæ™¯æµ‹è¯•å®Œæˆ")
        print(f"   æµ‹è¯•Promptæ•°: {total_prompts}")
        print(f"   æåŠç‡: {mention_rate:.2%}")
        print(f"   è¢«æåŠæ—¶å¹³å‡ç½®ä¿¡åº¦: {avg_confidence_when_mentioned:.2f}")
        print(f"   è¢«æåŠæ—¶å¹³å‡Prompté•¿åº¦: {avg_prompt_length_when_mentioned:.0f}å­—ç¬¦")
        
        # æ‰“å°æ¯ä¸ªPromptçš„ç»“æœ
        for i, result in enumerate(optimization_results):
            status = "âœ…" if result["mentioned"] else "âŒ"
            print(f"   Prompt {i+1} {status}: ç½®ä¿¡åº¦ {result['confidence']:.2f} - {result['prompt'][:50]}...")

    async def test_multi_model_consistency_scenario(
        self,
        skip_if_no_api_keys,
        mention_service,
        test_project_data,
        e2e_config
    ):
        """å¤šæ¨¡å‹ä¸€è‡´æ€§åœºæ™¯æµ‹è¯•"""
        print(f"ğŸ¤– å¼€å§‹å¤šæ¨¡å‹ä¸€è‡´æ€§åœºæ™¯æµ‹è¯•")
        
        # æµ‹è¯•åŒä¸€ä¸ªæŸ¥è¯¢åœ¨ä¸åŒæ¨¡å‹ä¸­çš„ä¸€è‡´æ€§
        test_queries = [
            "æ¨èå›¢é˜Ÿåä½œå·¥å…·",
            "æœ‰å“ªäº›å¥½ç”¨çš„ç¬”è®°è½¯ä»¶ï¼Ÿ"
        ]
        
        brands = ["Notion", "Obsidian"]
        models = ["doubao", "deepseek"]
        
        consistency_results = []
        
        for query in test_queries:
            print(f"   æµ‹è¯•æŸ¥è¯¢: {query}")

            # åˆ›å»ºæ£€æµ‹é…ç½®
            from app.services.mention_detection import MentionDetectionConfig
            config = MentionDetectionConfig(
                models=models,
                api_keys={
                    "DOUBAO_API_KEY": e2e_config["doubao_api_key"],
                    "DEEPSEEK_API_KEY": e2e_config["deepseek_api_key"]
                },
                max_tokens=300,
                temperature=0.3,
                parallel_execution=False
            )

            result = await mention_service.execute_detection(
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"],
                prompt=query,
                brands=brands,
                config=config
            )
            
            assert result.status == "completed"
            assert len(result.results) == len(models)
            
            # åˆ†ææ¨¡å‹é—´çš„ä¸€è‡´æ€§
            brand_consistency = {}
            
            for brand in brands:
                brand_results = []
                for model_result in result.results:
                    brand_mention = next(
                        (m for m in model_result.mentions if m.brand == brand), 
                        None
                    )
                    if brand_mention:
                        brand_results.append({
                            "model": model_result.model,
                            "mentioned": brand_mention.mentioned,
                            "confidence": brand_mention.confidence_score
                        })
                
                # è®¡ç®—ä¸€è‡´æ€§æŒ‡æ ‡
                mentioned_count = sum(1 for r in brand_results if r["mentioned"])
                consistency_rate = mentioned_count / len(brand_results) if brand_results else 0
                
                # ä¸€è‡´æ€§ï¼šè¦ä¹ˆéƒ½æåŠï¼Œè¦ä¹ˆéƒ½ä¸æåŠ
                is_consistent = mentioned_count == 0 or mentioned_count == len(brand_results)
                
                brand_consistency[brand] = {
                    "consistency_rate": consistency_rate,
                    "is_consistent": is_consistent,
                    "model_results": brand_results
                }
            
            consistency_results.append({
                "query": query,
                "brand_consistency": brand_consistency,
                "check_id": result.check_id
            })
        
        # éªŒè¯ä¸€è‡´æ€§ç»“æœ
        for result in consistency_results:
            for brand, consistency in result["brand_consistency"].items():
                assert 0 <= consistency["consistency_rate"] <= 1
                assert isinstance(consistency["is_consistent"], bool)
                assert len(consistency["model_results"]) == len(models)
        
        # è®¡ç®—æ€»ä½“ä¸€è‡´æ€§
        total_brand_tests = len(consistency_results) * len(brands)
        consistent_tests = sum(
            1 for result in consistency_results
            for brand_consistency in result["brand_consistency"].values()
            if brand_consistency["is_consistent"]
        )
        
        overall_consistency = consistent_tests / total_brand_tests if total_brand_tests > 0 else 0
        
        print(f"âœ… å¤šæ¨¡å‹ä¸€è‡´æ€§åœºæ™¯æµ‹è¯•å®Œæˆ")
        print(f"   æµ‹è¯•æŸ¥è¯¢æ•°: {len(test_queries)}")
        print(f"   æµ‹è¯•å“ç‰Œæ•°: {len(brands)}")
        print(f"   æ¨¡å‹æ•°: {len(models)}")
        print(f"   æ€»ä½“ä¸€è‡´æ€§: {overall_consistency:.2%}")
        
        # æ‰“å°è¯¦ç»†ä¸€è‡´æ€§ç»“æœ
        for result in consistency_results:
            print(f"   æŸ¥è¯¢: {result['query']}")
            for brand, consistency in result["brand_consistency"].items():
                status = "âœ…" if consistency["is_consistent"] else "âš ï¸"
                print(f"     {brand} {status}: ä¸€è‡´æ€§ç‡ {consistency['consistency_rate']:.2%}")
