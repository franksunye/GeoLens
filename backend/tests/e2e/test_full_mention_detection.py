"""
å®Œæ•´å¼•ç”¨æ£€æµ‹ä¸šåŠ¡æµç¨‹æµ‹è¯•
"""

import pytest
import asyncio
import json
from typing import List, Dict, Any

from app.services.mention_detection import MentionDetectionService
from app.repositories.mention_repository import MentionRepository


@pytest.mark.e2e
class TestFullMentionDetection:
    """æµ‹è¯•å®Œæ•´çš„å¼•ç”¨æ£€æµ‹ä¸šåŠ¡æµç¨‹"""

    async def test_end_to_end_detection_flow(
        self, 
        skip_if_no_api_keys, 
        mention_service, 
        test_brands, 
        test_project_data,
        e2e_db_session
    ):
        """ç«¯åˆ°ç«¯å¼•ç”¨æ£€æµ‹æµç¨‹æµ‹è¯•"""
        # æµ‹è¯•æ•°æ®
        prompt = "æ¨èå‡ ä¸ªé€‚åˆå›¢é˜Ÿåä½œçš„çŸ¥è¯†ç®¡ç†å·¥å…·"
        models = ["doubao", "deepseek"]
        
        print(f"ğŸš€ å¼€å§‹ç«¯åˆ°ç«¯å¼•ç”¨æ£€æµ‹æµ‹è¯•")
        print(f"   Prompt: {prompt}")
        print(f"   å“ç‰Œ: {test_brands}")
        print(f"   æ¨¡å‹: {models}")
        
        try:
            # 1. æ‰§è¡Œå¼•ç”¨æ£€æµ‹
            result = await mention_service.check_mentions(
                prompt=prompt,
                brands=test_brands,
                models=models,
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"]
            )
            
            # 2. éªŒè¯ç»“æœç»“æ„
            assert result is not None
            assert hasattr(result, 'check_id')
            assert hasattr(result, 'status')
            assert hasattr(result, 'results')
            assert hasattr(result, 'summary')
            
            # 3. éªŒè¯æ£€æµ‹çŠ¶æ€
            assert result.status == "completed"
            assert result.check_id is not None
            
            # 4. éªŒè¯æ¨¡å‹ç»“æœ
            assert len(result.results) == len(models)
            
            for model_result in result.results:
                assert hasattr(model_result, 'model')
                assert hasattr(model_result, 'response_text')
                assert hasattr(model_result, 'mentions')
                assert model_result.model in models
                assert len(model_result.response_text) > 0
                assert len(model_result.mentions) == len(test_brands)
                
                # éªŒè¯å“ç‰ŒæåŠç»“æœ
                for mention in model_result.mentions:
                    assert hasattr(mention, 'brand')
                    assert hasattr(mention, 'mentioned')
                    assert hasattr(mention, 'confidence_score')
                    assert mention.brand in test_brands
                    assert isinstance(mention.mentioned, bool)
                    assert 0 <= mention.confidence_score <= 1
            
            # 5. éªŒè¯æ±‡æ€»ä¿¡æ¯
            assert hasattr(result.summary, 'total_mentions')
            assert hasattr(result.summary, 'mention_rate')
            assert hasattr(result.summary, 'avg_confidence')
            assert result.summary.total_mentions >= 0
            assert 0 <= result.summary.mention_rate <= 1
            assert 0 <= result.summary.avg_confidence <= 1
            
            # 6. éªŒè¯æ•°æ®åº“æŒä¹…åŒ–
            repo = MentionRepository(e2e_db_session)
            saved_check = await repo.get_check_by_id(result.check_id)
            
            assert saved_check is not None
            assert saved_check["id"] == result.check_id
            assert saved_check["status"] == "completed"
            assert saved_check["prompt"] == prompt
            
            # éªŒè¯å“ç‰Œå’Œæ¨¡å‹ä¿¡æ¯
            saved_brands = json.loads(saved_check["brands_checked"])
            saved_models = json.loads(saved_check["models_used"])
            assert set(saved_brands) == set(test_brands)
            assert set(saved_models) == set(models)
            
            print(f"âœ… ç«¯åˆ°ç«¯æ£€æµ‹æµç¨‹æµ‹è¯•æˆåŠŸ")
            print(f"   æ£€æµ‹ID: {result.check_id}")
            print(f"   æ€»æåŠæ•°: {result.summary.total_mentions}")
            print(f"   æåŠç‡: {result.summary.mention_rate:.2%}")
            print(f"   å¹³å‡ç½®ä¿¡åº¦: {result.summary.avg_confidence:.2f}")
            
        except Exception as e:
            pytest.fail(f"ç«¯åˆ°ç«¯æ£€æµ‹æµç¨‹æµ‹è¯•å¤±è´¥: {str(e)}")

    async def test_multi_brand_detection(
        self, 
        skip_if_no_api_keys, 
        mention_service, 
        test_project_data
    ):
        """å¤šå“ç‰Œæ£€æµ‹æµ‹è¯•"""
        # ä½¿ç”¨æ›´å¤šå“ç‰Œè¿›è¡Œæµ‹è¯•
        brands = ["Notion", "Obsidian", "Roam Research", "Logseq", "Craft"]
        prompt = "æ¯”è¾ƒå‡ ä¸ªçŸ¥è¯†ç®¡ç†å·¥å…·çš„ä¼˜ç¼ºç‚¹"
        models = ["doubao"]  # ä½¿ç”¨å•ä¸ªæ¨¡å‹ä»¥å‡å°‘APIè°ƒç”¨
        
        print(f"ğŸ” å¼€å§‹å¤šå“ç‰Œæ£€æµ‹æµ‹è¯•")
        print(f"   å“ç‰Œæ•°é‡: {len(brands)}")
        
        try:
            result = await mention_service.check_mentions(
                prompt=prompt,
                brands=brands,
                models=models,
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"]
            )
            
            # éªŒè¯æ‰€æœ‰å“ç‰Œéƒ½è¢«æ£€æµ‹
            assert len(result.results) == 1  # ä¸€ä¸ªæ¨¡å‹
            model_result = result.results[0]
            assert len(model_result.mentions) == len(brands)
            
            # éªŒè¯æ¯ä¸ªå“ç‰Œéƒ½æœ‰æ£€æµ‹ç»“æœ
            detected_brands = {mention.brand for mention in model_result.mentions}
            assert detected_brands == set(brands)
            
            # ç»Ÿè®¡è¢«æåŠçš„å“ç‰Œ
            mentioned_brands = [
                mention.brand for mention in model_result.mentions 
                if mention.mentioned
            ]
            
            print(f"âœ… å¤šå“ç‰Œæ£€æµ‹æµ‹è¯•æˆåŠŸ")
            print(f"   æ£€æµ‹å“ç‰Œ: {len(brands)}")
            print(f"   è¢«æåŠå“ç‰Œ: {len(mentioned_brands)}")
            print(f"   è¢«æåŠçš„å“ç‰Œ: {mentioned_brands}")
            
        except Exception as e:
            pytest.fail(f"å¤šå“ç‰Œæ£€æµ‹æµ‹è¯•å¤±è´¥: {str(e)}")

    async def test_real_world_scenarios(
        self, 
        skip_if_no_api_keys, 
        mention_service, 
        test_prompts, 
        test_project_data
    ):
        """çœŸå®ä¸–ç•Œåœºæ™¯æµ‹è¯•"""
        brands = ["Notion", "Obsidian"]
        models = ["doubao", "deepseek"]
        
        print(f"ğŸŒ å¼€å§‹çœŸå®ä¸–ç•Œåœºæ™¯æµ‹è¯•")
        
        scenario_results = {}
        
        for scenario_name, prompt in test_prompts.items():
            print(f"   æµ‹è¯•åœºæ™¯: {scenario_name}")
            
            try:
                result = await mention_service.check_mentions(
                    prompt=prompt,
                    brands=brands,
                    models=models,
                    project_id=test_project_data["project_id"],
                    user_id=test_project_data["user_id"]
                )
                
                # æ”¶é›†åœºæ™¯ç»“æœ
                scenario_results[scenario_name] = {
                    "total_mentions": result.summary.total_mentions,
                    "mention_rate": result.summary.mention_rate,
                    "avg_confidence": result.summary.avg_confidence,
                    "models_count": len(result.results)
                }
                
                # åŸºæœ¬éªŒè¯
                assert result.status == "completed"
                assert len(result.results) == len(models)
                
                print(f"     âœ… {scenario_name}: æåŠç‡ {result.summary.mention_rate:.2%}")
                
            except Exception as e:
                pytest.fail(f"åœºæ™¯ {scenario_name} æµ‹è¯•å¤±è´¥: {str(e)}")
        
        # éªŒè¯æ‰€æœ‰åœºæ™¯éƒ½æœ‰ç»“æœ
        assert len(scenario_results) == len(test_prompts)
        
        # æ‰“å°æ±‡æ€»ç»“æœ
        print(f"âœ… çœŸå®ä¸–ç•Œåœºæ™¯æµ‹è¯•å®Œæˆ")
        for scenario, stats in scenario_results.items():
            print(f"   {scenario}: æåŠæ•°={stats['total_mentions']}, "
                  f"æåŠç‡={stats['mention_rate']:.2%}, "
                  f"ç½®ä¿¡åº¦={stats['avg_confidence']:.2f}")

    async def test_error_recovery(
        self, 
        skip_if_no_api_keys, 
        mention_service, 
        test_project_data
    ):
        """é”™è¯¯æ¢å¤æµ‹è¯•"""
        print(f"ğŸ”§ å¼€å§‹é”™è¯¯æ¢å¤æµ‹è¯•")
        
        # æµ‹è¯•ç©ºå“ç‰Œåˆ—è¡¨
        try:
            result = await mention_service.check_mentions(
                prompt="æµ‹è¯•ç©ºå“ç‰Œåˆ—è¡¨",
                brands=[],  # ç©ºå“ç‰Œåˆ—è¡¨
                models=["doubao"],
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"]
            )
            
            # åº”è¯¥èƒ½æ­£å¸¸å¤„ç†ç©ºå“ç‰Œåˆ—è¡¨
            assert result.status == "completed"
            assert len(result.results) == 1
            assert len(result.results[0].mentions) == 0
            
            print(f"   âœ… ç©ºå“ç‰Œåˆ—è¡¨å¤„ç†æ­£å¸¸")
            
        except Exception as e:
            pytest.fail(f"ç©ºå“ç‰Œåˆ—è¡¨æµ‹è¯•å¤±è´¥: {str(e)}")
        
        # æµ‹è¯•ç©ºPrompt
        try:
            result = await mention_service.check_mentions(
                prompt="",  # ç©ºPrompt
                brands=["Notion"],
                models=["doubao"],
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"]
            )
            
            # åº”è¯¥èƒ½å¤„ç†ç©ºPrompt
            assert result.status == "completed"
            
            print(f"   âœ… ç©ºPromptå¤„ç†æ­£å¸¸")
            
        except Exception as e:
            # ç©ºPromptå¯èƒ½ä¼šå¯¼è‡´é”™è¯¯ï¼Œè¿™æ˜¯å¯ä»¥æ¥å—çš„
            print(f"   âš ï¸ ç©ºPromptå¯¼è‡´é”™è¯¯ï¼ˆå¯æ¥å—ï¼‰: {str(e)}")
        
        print(f"âœ… é”™è¯¯æ¢å¤æµ‹è¯•å®Œæˆ")
