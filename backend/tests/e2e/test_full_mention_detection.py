"""
å®Œæ•´å¼•ç”¨æ£€æµ‹ä¸šåŠ¡æµç¨‹æµ‹è¯•
"""

import pytest
import asyncio
import json
from typing import List, Dict, Any

from app.services.mention_detection import MentionDetectionService
from app.repositories.mention_repository import MentionRepository


@pytest.mark.e2e
class TestFullMentionDetection:
    """æµ‹è¯•å®Œæ•´çš„å¼•ç”¨æ£€æµ‹ä¸šåŠ¡æµç¨‹"""

    async def test_end_to_end_detection_flow(
        self,
        skip_if_no_api_keys,
        mention_service,
        test_brands,
        test_project_data,
        e2e_db_session,
        e2e_config
    ):
        """ç«¯åˆ°ç«¯å¼•ç”¨æ£€æµ‹æµç¨‹æµ‹è¯•"""
        # æµ‹è¯•æ•°æ®
        prompt = "æ¨èå‡ ä¸ªé€‚åˆå›¢é˜Ÿåä½œçš„çŸ¥è¯†ç®¡ç†å·¥å…·"
        models = ["doubao", "deepseek"]
        
        print(f"ğŸš€ å¼€å§‹ç«¯åˆ°ç«¯å¼•ç”¨æ£€æµ‹æµ‹è¯•")
        print(f"   Prompt: {prompt}")
        print(f"   å“ç‰Œ: {test_brands}")
        print(f"   æ¨¡å‹: {models}")
        
        try:
            # 1. åˆ›å»ºæ£€æµ‹é…ç½®
            from app.services.mention_detection import MentionDetectionConfig
            config = MentionDetectionConfig(
                models=models,
                api_keys={
                    "DOUBAO_API_KEY": e2e_config["doubao_api_key"],
                    "DEEPSEEK_API_KEY": e2e_config["deepseek_api_key"]
                },
                max_tokens=300,
                temperature=0.3,
                parallel_execution=False  # ä¸²è¡Œæ‰§è¡Œä»¥å‡å°‘APIè°ƒç”¨
            )

            # 2. æ‰§è¡Œå¼•ç”¨æ£€æµ‹
            result = await mention_service.execute_detection(
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"],
                prompt=prompt,
                brands=test_brands,
                config=config
            )
            
            # 3. éªŒè¯ç»“æœç»“æ„
            assert result is not None
            assert hasattr(result, 'check_id')
            assert hasattr(result, 'status')
            assert hasattr(result, 'results')
            assert hasattr(result, 'summary')

            # 4. éªŒè¯æ£€æµ‹çŠ¶æ€
            assert result.status == "completed"
            assert result.check_id is not None

            # 5. éªŒè¯æ¨¡å‹ç»“æœ
            assert len(result.results) == len(models)
            
            for model_result in result.results:
                assert hasattr(model_result, 'model')
                assert hasattr(model_result, 'response_text')
                assert hasattr(model_result, 'mentions')
                assert model_result.model in models
                # æ³¨æ„ï¼šæŸäº›AIæ¨¡å‹å¯èƒ½è¿”å›ç©ºå“åº”ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                assert model_result.response_text is not None
                assert len(model_result.mentions) == len(test_brands)
                
                # éªŒè¯å“ç‰ŒæåŠç»“æœ
                for mention in model_result.mentions:
                    assert hasattr(mention, 'brand')
                    assert hasattr(mention, 'mentioned')
                    assert hasattr(mention, 'confidence_score')
                    assert mention.brand in test_brands
                    assert isinstance(mention.mentioned, bool)
                    assert 0 <= mention.confidence_score <= 1
            
            # 6. éªŒè¯æ±‡æ€»ä¿¡æ¯
            assert isinstance(result.summary, dict)
            assert 'total_mentions' in result.summary
            assert 'mention_rate' in result.summary
            assert 'avg_confidence' in result.summary
            assert result.summary['total_mentions'] >= 0
            assert 0 <= result.summary['mention_rate'] <= 1
            assert 0 <= result.summary['avg_confidence'] <= 1
            
            # 7. éªŒè¯æ•°æ®åº“æŒä¹…åŒ–
            from app.repositories.mention_repository import MentionRepository
            repo = MentionRepository(e2e_db_session)
            saved_check = await repo.get_check_by_id(result.check_id)

            assert saved_check is not None
            assert saved_check.id == result.check_id
            assert saved_check.status == "completed"
            assert saved_check.prompt == prompt

            # éªŒè¯å“ç‰Œå’Œæ¨¡å‹ä¿¡æ¯
            saved_brands = json.loads(saved_check.brands_checked)
            saved_models = json.loads(saved_check.models_used)
            assert set(saved_brands) == set(test_brands)
            assert set(saved_models) == set(models)
            
            print(f"âœ… ç«¯åˆ°ç«¯æ£€æµ‹æµç¨‹æµ‹è¯•æˆåŠŸ")
            print(f"   æ£€æµ‹ID: {result.check_id}")
            print(f"   æ€»æåŠæ•°: {result.summary['total_mentions']}")
            print(f"   æåŠç‡: {result.summary['mention_rate']:.2%}")
            print(f"   å¹³å‡ç½®ä¿¡åº¦: {result.summary['avg_confidence']:.2f}")
            
        except Exception as e:
            pytest.fail(f"ç«¯åˆ°ç«¯æ£€æµ‹æµç¨‹æµ‹è¯•å¤±è´¥: {str(e)}")

    async def test_multi_brand_detection(
        self,
        skip_if_no_api_keys,
        mention_service,
        test_project_data,
        e2e_config
    ):
        """å¤šå“ç‰Œæ£€æµ‹æµ‹è¯•"""
        # ä½¿ç”¨æ›´å¤šå“ç‰Œè¿›è¡Œæµ‹è¯•
        brands = ["Notion", "Obsidian", "Roam Research", "Logseq", "Craft"]
        prompt = "æ¯”è¾ƒå‡ ä¸ªçŸ¥è¯†ç®¡ç†å·¥å…·çš„ä¼˜ç¼ºç‚¹"
        models = ["doubao"]  # ä½¿ç”¨å•ä¸ªæ¨¡å‹ä»¥å‡å°‘APIè°ƒç”¨
        
        print(f"ğŸ” å¼€å§‹å¤šå“ç‰Œæ£€æµ‹æµ‹è¯•")
        print(f"   å“ç‰Œæ•°é‡: {len(brands)}")
        
        try:
            # åˆ›å»ºæ£€æµ‹é…ç½®
            from app.services.mention_detection import MentionDetectionConfig
            config = MentionDetectionConfig(
                models=models,
                api_keys={
                    "DOUBAO_API_KEY": e2e_config["doubao_api_key"],
                    "DEEPSEEK_API_KEY": e2e_config["deepseek_api_key"]
                },
                max_tokens=300,
                temperature=0.3,
                parallel_execution=False
            )

            result = await mention_service.execute_detection(
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"],
                prompt=prompt,
                brands=brands,
                config=config
            )
            
            # éªŒè¯æ‰€æœ‰å“ç‰Œéƒ½è¢«æ£€æµ‹
            assert len(result.results) == 1  # ä¸€ä¸ªæ¨¡å‹
            model_result = result.results[0]
            assert len(model_result.mentions) == len(brands)
            
            # éªŒè¯æ¯ä¸ªå“ç‰Œéƒ½æœ‰æ£€æµ‹ç»“æœ
            detected_brands = {mention.brand for mention in model_result.mentions}
            assert detected_brands == set(brands)
            
            # ç»Ÿè®¡è¢«æåŠçš„å“ç‰Œ
            mentioned_brands = [
                mention.brand for mention in model_result.mentions 
                if mention.mentioned
            ]
            
            print(f"âœ… å¤šå“ç‰Œæ£€æµ‹æµ‹è¯•æˆåŠŸ")
            print(f"   æ£€æµ‹å“ç‰Œ: {len(brands)}")
            print(f"   è¢«æåŠå“ç‰Œ: {len(mentioned_brands)}")
            print(f"   è¢«æåŠçš„å“ç‰Œ: {mentioned_brands}")
            
        except Exception as e:
            pytest.fail(f"å¤šå“ç‰Œæ£€æµ‹æµ‹è¯•å¤±è´¥: {str(e)}")

    async def test_real_world_scenarios(
        self,
        skip_if_no_api_keys,
        mention_service,
        test_prompts,
        test_project_data,
        e2e_config
    ):
        """çœŸå®ä¸–ç•Œåœºæ™¯æµ‹è¯•"""
        brands = ["Notion", "Obsidian"]
        models = ["doubao", "deepseek"]
        
        print(f"ğŸŒ å¼€å§‹çœŸå®ä¸–ç•Œåœºæ™¯æµ‹è¯•")
        
        scenario_results = {}
        
        for scenario_name, prompt in test_prompts.items():
            print(f"   æµ‹è¯•åœºæ™¯: {scenario_name}")
            
            try:
                # åˆ›å»ºæ£€æµ‹é…ç½®
                from app.services.mention_detection import MentionDetectionConfig
                config = MentionDetectionConfig(
                    models=models,
                    api_keys={
                        "DOUBAO_API_KEY": e2e_config["doubao_api_key"],
                        "DEEPSEEK_API_KEY": e2e_config["deepseek_api_key"]
                    },
                    max_tokens=300,
                    temperature=0.3,
                    parallel_execution=False
                )

                result = await mention_service.execute_detection(
                    project_id=test_project_data["project_id"],
                    user_id=test_project_data["user_id"],
                    prompt=prompt,
                    brands=brands,
                    config=config
                )

                # æ”¶é›†åœºæ™¯ç»“æœ
                scenario_results[scenario_name] = {
                    "total_mentions": result.summary["total_mentions"],
                    "mention_rate": result.summary["mention_rate"],
                    "avg_confidence": result.summary["avg_confidence"],
                    "models_count": len(result.results)
                }

                # åŸºæœ¬éªŒè¯
                assert result.status == "completed"
                assert len(result.results) == len(models)

                print(f"     âœ… {scenario_name}: æåŠç‡ {result.summary['mention_rate']:.2%}")
                
            except Exception as e:
                pytest.fail(f"åœºæ™¯ {scenario_name} æµ‹è¯•å¤±è´¥: {str(e)}")
        
        # éªŒè¯æ‰€æœ‰åœºæ™¯éƒ½æœ‰ç»“æœ
        assert len(scenario_results) == len(test_prompts)
        
        # æ‰“å°æ±‡æ€»ç»“æœ
        print(f"âœ… çœŸå®ä¸–ç•Œåœºæ™¯æµ‹è¯•å®Œæˆ")
        for scenario, stats in scenario_results.items():
            print(f"   {scenario}: æåŠæ•°={stats['total_mentions']}, "
                  f"æåŠç‡={stats['mention_rate']:.2%}, "
                  f"ç½®ä¿¡åº¦={stats['avg_confidence']:.2f}")

    async def test_error_recovery(
        self,
        skip_if_no_api_keys,
        mention_service,
        test_project_data,
        e2e_config
    ):
        """é”™è¯¯æ¢å¤æµ‹è¯•"""
        print(f"ğŸ”§ å¼€å§‹é”™è¯¯æ¢å¤æµ‹è¯•")
        
        # æµ‹è¯•ç©ºå“ç‰Œåˆ—è¡¨
        try:
            from app.services.mention_detection import MentionDetectionConfig
            config = MentionDetectionConfig(
                models=["doubao"],
                api_keys={"DOUBAO_API_KEY": e2e_config["doubao_api_key"]},
                max_tokens=300,
                temperature=0.3,
                parallel_execution=False
            )

            result = await mention_service.execute_detection(
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"],
                prompt="æµ‹è¯•ç©ºå“ç‰Œåˆ—è¡¨",
                brands=[],  # ç©ºå“ç‰Œåˆ—è¡¨
                config=config
            )
            
            # åº”è¯¥èƒ½æ­£å¸¸å¤„ç†ç©ºå“ç‰Œåˆ—è¡¨
            assert result.status == "completed"
            assert len(result.results) == 1
            assert len(result.results[0].mentions) == 0
            
            print(f"   âœ… ç©ºå“ç‰Œåˆ—è¡¨å¤„ç†æ­£å¸¸")
            
        except Exception as e:
            pytest.fail(f"ç©ºå“ç‰Œåˆ—è¡¨æµ‹è¯•å¤±è´¥: {str(e)}")
        
        # æµ‹è¯•ç©ºPrompt
        try:
            config = MentionDetectionConfig(
                models=["doubao"],
                api_keys={"DOUBAO_API_KEY": e2e_config["doubao_api_key"]},
                max_tokens=300,
                temperature=0.3,
                parallel_execution=False
            )

            result = await mention_service.execute_detection(
                project_id=test_project_data["project_id"],
                user_id=test_project_data["user_id"],
                prompt="",  # ç©ºPrompt
                brands=["Notion"],
                config=config
            )
            
            # åº”è¯¥èƒ½å¤„ç†ç©ºPrompt
            assert result.status == "completed"
            
            print(f"   âœ… ç©ºPromptå¤„ç†æ­£å¸¸")
            
        except Exception as e:
            # ç©ºPromptå¯èƒ½ä¼šå¯¼è‡´é”™è¯¯ï¼Œè¿™æ˜¯å¯ä»¥æ¥å—çš„
            print(f"   âš ï¸ ç©ºPromptå¯¼è‡´é”™è¯¯ï¼ˆå¯æ¥å—ï¼‰: {str(e)}")
        
        print(f"âœ… é”™è¯¯æ¢å¤æµ‹è¯•å®Œæˆ")
